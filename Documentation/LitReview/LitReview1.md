Great, I’ll start compiling a list of relevant and recent academic papers (along with foundational ones where useful) related to vision-language models, wearable assistive technology, real-time scene understanding, edge AI (especially on Raspberry Pi), and TTS systems. I’ll ensure all papers include direct open-access PDF links. Once that’s ready, I’ll also prepare a thorough literature review summarizing key findings, technologies, and trends.

I’ll keep you posted once the papers and review are ready for your review.


# AIris Project: State-of-the-Art Review and Relevant Works

**Note:** *This report provides a structured literature review for the AIris project – a wearable real-time scene description system for the visually impaired – covering recent vision-language models, wearable assistive devices, edge AI deployment, embedded computer vision/NLP, real-time object detection with contextual understanding, and text-to-speech systems for accessibility. Key academic papers (with open-access PDFs) are listed first, followed by a comprehensive review with implementation insights and benchmarks.*

## Relevant Papers (Open-Access PDFs)

1. **Brilli *et al.* (2024)** – *“AIris: An AI-powered Wearable Assistive Device for the Visually Impaired.”* ArXiv preprint.  This paper introduces **AIris**, a prototype smart glasses system (camera + Raspberry Pi + cloud) providing real-time object recognition, scene description, text reading, face recognition, money counting, and more for blind users.
2. **Baig *et al.* (2024)** – *“AI-based Wearable Vision Assistance System for the Visually Impaired: Integrating Real-Time Object Recognition and Contextual Understanding Using Large Vision-Language Models.”* ArXiv preprint.  Proposes a hat-mounted camera with a Raspberry Pi 4 that uses onboard object detection and **Large Vision-Language Models (LVLMs)** for rich scene descriptions, plus an ultrasonic sensor for obstacle avoidance. Evaluates user satisfaction and system performance.
3. **Sethuraman *et al.* (2023)** – *“MagicEye: An Intelligent Wearable Towards Independent Living of Visually Impaired.”* ArXiv preprint.  Presents **MagicEye**, a wearable device with a custom CNN-based object detector (35 classes), facial recognition, currency detection, GPS for navigation, and a proximity sensor. Emphasizes an efficient embedded design for indoor/outdoor object recognition and navigation assistance.
4. **Bobba *et al.* (2023)** – *“NewVision: Application for Helping Blind People Using Deep Learning.”* ArXiv preprint.  Describes a conceptual **headgear** system that integrates computer vision, ultrasonic rangefinders, voice command recognition, and a voice assistant. Users can ask questions (e.g. “What is that?”) and receive spoken scene descriptions or navigation directions, illustrating a modular approach to assistive AI.
5. **Li *et al.* (2023)** – *“BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.”* ArXiv preprint.  Proposes **BLIP-2**, a state-of-the-art vision-language model that connects a frozen image encoder to a frozen large language model via a lightweight transformer. Achieves strong image captioning and VQA performance with far fewer trainable parameters than prior methods.
6. **Zhu *et al.* (2023)** – *“MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.”* ArXiv preprint.  Introduces **MiniGPT-4**, which aligns a frozen visual encoder (from BLIP-2) with a powerful language model (Vicuna) using a single projection layer. Demonstrates advanced multimodal capabilities similar to GPT-4, such as generating detailed image descriptions and answering visual queries.
7. **Liu *et al.* (2023)** – *“Visual Instruction Tuning” (LLaVA: Large Language-and-Vision Assistant).* NeurIPS 2023 (Oral).  Develops **LLaVA**, an end-to-end multimodal model that connects a vision encoder to a language model and is instruction-tuned using GPT-4 generated data. LLaVA exhibits impressive image-based conversational abilities, achieving \~85% of GPT-4’s performance on a test benchmark.

*(Additional foundational works and surveys are cited within the review below for context.)*

## Introduction and Background

According to the World Health Organization, over **2.2 billion people** worldwide live with some form of visual impairment. This enormous population faces daily challenges in navigation, object recognition, reading text, and social interaction due to lack of visual feedback. In recent years, **AI-powered assistive technologies** have advanced significantly to address these challenges. Wearable systems, in particular, offer a promising solution by providing visually impaired users with real-time auditory descriptions of their surroundings, essentially acting as an artificial “guide” or **digital sight**. Early electronic travel aids (e.g. smart canes, talking GPS devices) provided basic obstacle detection or navigation cues, but **modern approaches strive for comprehensive scene understanding** and richer contextual feedback. This review surveys the state-of-the-art relevant to **AIris**, a newly proposed wearable scene description system. We cover vision-language models for scene description, wearable assistive device designs, edge AI deployment on devices like Raspberry Pi, embedded computer vision and NLP techniques, real-time object detection with context, and text-to-speech (TTS) systems optimized for accessibility.

## Vision-Language Models for Real-Time Scene Description

A core component of AIris is the **scene description module**, which generates natural-language descriptions of a camera’s view in real time. Recent **vision-language models (VLMs)** have made huge strides in image captioning and visual question answering, enabling far more detailed and context-aware descriptions than earlier fixed-vocabulary models. For example, the **BLIP-2** framework (Bootstrapping Language-Image Pre-training) connects a frozen image encoder (e.g. ViT visual transformer) with a frozen language model using a lightweight Query Transformer. By avoiding end-to-end training of a giant multimodal model, BLIP-2 achieves state-of-the-art results on captioning and VQA with drastically fewer trainable parameters – it even outperforms a 80B-parameter Flamingo model on zero-shot VQA by 8.7%, using 54× fewer learned parameters. This efficiency makes BLIP-2 attractive for real-time or edge scenarios.

Building on such advances, researchers have begun **aligning vision encoders with large LLMs** to emulate the multimodal prowess of GPT-4. **MiniGPT-4** is a prominent example: it takes the pre-trained visual encoder and Q-Former from BLIP-2 and **pairs it with Vicuna (a GPT-4-quality language model)** via a simple linear projection layer. Remarkably, with only \~10 hours of additional training, this system acquires many of GPT-4’s “emergent” vision-language abilities – generating rich image descriptions, explaining visual jokes, writing stories about an image, etc.. MiniGPT-4’s success demonstrates that **a frozen advanced LLM can be leveraged for vision tasks** if given appropriate visual features. Similarly, **LLaVA (Large Language-and-Vision Assistant)** was trained via *visual instruction tuning*, where GPT-4 was used to generate a large set of image-instruction-response examples. After fine-tuning on these, LLaVA can engage in dialogue about images and follow open-ended visual instructions, achieving about 85% of GPT-4’s performance on a test set. These models (and related ones like InstructBLIP, CM3, etc.) represent the **state of the art in vision-language understanding**.

For AIris, such models are highly relevant: they could enable the device not only to list objects but to **describe scenes in human-like detail** (“a person sitting at a table reading a menu in a cafe”) and answer spoken questions about the scene. However, a direct deployment of these models is challenging on a wearable – they typically require significant memory and GPU computation. A practical approach is to use them via a cloud API or to distill/compress them. Indeed, the current AIris prototype uses a simpler image captioning network (based on Inception-v3 and an LSTM decoder) from IBM’s Model Asset eXchange, achieving 27% BLEU on Flickr8k. The authors explicitly note that **integrating advanced LLMs and generative models is the next step** to improve AIris’s capabilities. In a newer 2024 study, Baig *et al.* took this step by incorporating a **Large Vision-Language Model (LVLM)** into a wearable system to generate “detailed explanations and relevant information about recognized items,” providing a *richer contextual understanding* of the environment. This demonstrates how the latest VLMs can be harnessed to greatly enhance scene description for assistive devices.

## Wearable Assistive Technologies for the Visually Impaired

A number of wearable systems have been proposed to assist blind or low-vision users, combining cameras, sensors, and AI to augment perception. These range from experimental academic prototypes like AIris to commercial products like OrCam MyEye. **AIris** itself is a prime example of a multi-function wearable: it consists of a camera mounted on 3D-printed eyeglasses, bone-conduction earphones, and a Raspberry Pi microcomputer. The design emphasizes user comfort (distributing weight by keeping the Pi in the user’s pocket) and modularity (each function is a separate “module” that can be updated or replaced). AIris supports a broad suite of tasks – object detection, scene captioning, reading printed text (OCR), recognizing faces, identifying currency, taking notes via voice, and barcode scanning. In an informal trial with visually impaired users, these features were well-received and provided a **sense of security and independence**, though issues like response latency and ergonomics were noted for improvement.

Other contemporary projects focus on similar goals. **MagicEye** (2023) is an “intelligent wearable” that also targets **indoor and outdoor object recognition, navigation, and reading assistance**. It employs a custom-trained CNN that can detect 35 common object classes with high efficiency, along with modules for face recognition and currency identification. MagicEye further integrates a **GPS sensor for navigation** and a proximity (ultrasonic) sensor for obstacle avoidance, warning the user of nearby objects without physical contact. This suggests a trend of combining **computer vision with additional sensors** (GPS, sonar) to provide both vision-based understanding and spatial awareness. Another system, **NewVision** (Bobba *et al.* 2023), envisions a head-mounted device that can handle a wide range of tasks (navigation, object ID, text reading, etc.) via separate AI modules. Notably, NewVision emphasizes voice interaction: users can issue natural voice commands (e.g. asking “What is that?” when they hear something nearby) and the system will respond with a spoken description. It essentially serves as a voice-driven “AI assistant” dedicated to the blind, combining vision, voice recognition, and an audio interface. This aligns with efforts like Microsoft’s Seeing AI app and the Be My Eyes AI assistant, except in a hands-free wearable format.

Academic research has also explored **smart cane and smart cap** solutions. For instance, Levin & Mitra (2022) attached a Jetson Nano (a small GPU-enabled computer) and camera to the handle of a white cane, performing real-time object detection (with an SSD-Inception model) on the cane and then **wirelessly relaying spoken results to a smartphone**. By augmenting a familiar mobility aid (the white cane) with vision capabilities, their system provides object identification on the go without requiring the user to wear glasses or a harness. Others have worked on **smart caps or hats** equipped with cameras and ultrasonic sensors, which alert the user to obstacles via audio or vibration and use image recognition to name objects or read signs. Wearable designs thus vary from glasses to hats to canes, but all seek to seamlessly integrate into a visually impaired person’s daily life. Key design considerations include **comfort (weight distribution, heat), hands-free operation, and inconspicuous form-factor**, as well as **battery life** sufficient for all-day use.

In summary, modern wearable assistive devices strive to be **multifunctional “seeing” devices** for the blind, going beyond simple obstacle detection. Projects like AIris and MagicEye demonstrate that it is now feasible to pack multiple deep-learning functions (object detection, OCR, face ID, etc.) into a wearable kit, thanks to small but powerful computers (e.g. Raspberry Pi 4, Jetson) and cloud connectivity. User studies so far (though limited) indicate these devices can significantly enhance users’ environmental awareness, confidence, and autonomy. As hardware improves and AI models become more efficient, we expect such wearables to become lighter, faster, and more capable, moving closer to mainstream adoption for assistive technology.

## Edge AI Deployment on Raspberry Pi and Embedded Systems

Deploying AI algorithms on **resource-constrained devices** is a central challenge for wearable assistive tech. Raspberry Pi and similar single-board computers provide an attractive platform due to their low cost, small size, and community support. However, they have limited processing power, memory, and energy compared to cloud servers or desktops. Developers have adopted several strategies to enable real-time AI on the edge. One common approach is to use **optimized, lightweight models** for on-device tasks. For example, Baig *et al.* use **MobileNet-SSD** – a streamlined object detection network – and **FaceNet** – an efficient face recognition model – on the Raspberry Pi 4 to perform local detection/recognition. Both models are converted to TensorFlow Lite and **quantized** (reduced precision) to shrink their size and increase inference speed without significant loss of accuracy. By limiting image resolution (e.g. 300×300 input for object detection) and focusing on essential classes, they achieve real-time performance on the Pi. Similarly, MagicEye’s 35-class CNN detector was likely designed with a small architecture tailored for the embedded processor. These examples show how **model compression and efficient architectures** enable edge AI on wearables.

Another technique is to exploit hardware parallelism on these boards. The Raspberry Pi 4, for instance, has a 4-core CPU and a modest GPU. Baig’s system pipelines tasks on separate threads to use all CPU cores and even offloads some operations (like image capture or simple image processing) to the GPU, thereby avoiding bottlenecks on any single thread. Careful scheduling and resource monitoring help maintain a stable frame rate and prevent overloading the Pi. Power management (e.g. turning off camera or screen when not needed) is also important for battery-operated use. Despite these optimizations, it’s often not feasible to run *heavy* deep models (like a transformer-based image captioner or large language model) fully on the device. Hence, many systems adopt a **hybrid edge-cloud architecture**: the wearable performs lightweight tasks locally and sends heavier tasks to a server. AIris exemplifies this: the **Raspberry Pi handles local data capture and user interaction**, but it transmits images to a remote server for computationally intensive inference (object detection, scene analysis) on more powerful hardware. Once the server returns the results, the Pi then assembles the final spoken feedback for the user. This distributed design balances speed and accuracy – the user gets results with minimal latency, and the heavy lifting is done in the cloud where large models can run. The trade-off is reliance on a network connection, which may be intermittent in real-world mobility scenarios.

For purely offline use, some projects turn to more capable edge devices like the **NVIDIA Jetson family**, which include GPU acceleration. The Jetson Nano (with a 128-core GPU) has been used to run YOLO or SSD detectors at reasonable speeds, as in the smart cane project where the Nano processed video frames and sent labels to a phone. Newer Jetson models (Xavier, Orin) could potentially run medium-sized vision transformers or even small vision-language models on-device, though at higher cost and power consumption. Another emerging option is specialized AI accelerators (Google Coral TPU, Intel Neural Compute Stick) that can be attached to boards like the Pi to speed up inference of neural networks. In summary, **edge AI deployment** requires carefully matching the model to the hardware: simpler models (e.g. MobileNet, YOLO-Tiny) can run locally on Pi/Jetson for real-time detection, whereas complex models (image captioners, generative transformers) may need to be accessed via cloud services due to device limitations. Researchers are actively working on techniques like model distillation, quantization, and efficient network design to bring more AI capabilities onto the edge devices themselves, which would improve reliability and privacy by reducing cloud dependence.

## Real-Time Object Detection and Contextual Understanding

At the heart of any scene description system is **real-time object detection** – the ability to identify what objects are present in the camera’s view (and possibly their location). Modern assistive devices predominantly leverage deep learning-based detectors. **YOLO (You Only Look Once)** is a popular choice for its speed and decent accuracy; AIris uses YOLO on the server side to detect and count objects in the scene, feeding that information into its response generation. The version of YOLO isn’t specified, but even YOLOv3 or v4 can run at >20 FPS on a GPU, making it suitable for near-real-time feedback. YOLO achieved about 63.4% mAP on the COCO benchmark in AIris’s configuration, indicating robust performance on a wide range of object categories. Other projects, as noted, use **SSD (Single Shot Detector)** variants – for example, MobileNet-SSD in Baig’s system and SSD-Inception in the Jetson cane – which can run on CPUs or small GPUs. These detectors typically provide the *names* of objects (e.g. “person”, “chair”, “car”) and bounding boxes. On their own, they give the user a list of nearby objects; this itself is very useful (knowing that “a car is approaching from the left” or “there is a chair in front of you” is critical for safe navigation). However, simply enumerating objects may not give a full **contextual picture** of the scene. This is where higher-level reasoning and scene description come in.

**Contextual understanding** involves answering: *How are these objects related? What activity or scene do they form?* For instance, seeing objects “person, chair, table, book” could be summarized as “a person reading at a table.” The early version of AIris approached this via an image captioning model (MAX Image Caption Generator), which produces a basic sentence about the image. While useful, that model was limited to patterns seen in training (MS COCO dataset) and had a fixed vocabulary. In contrast, integrating a **vision-language model or an LLM** allows much more flexible and detailed descriptions. Baig *et al.* demonstrate this by sending the recognized object labels and other cues from the Pi to an **OpenAI GPT-4 model via API**, effectively asking the LLM to compose a descriptive sentence or answer a question about the scene. In their system, the user presses a dedicated button when they want a *detailed explanation* of the scene; the system then gathers the current context (detected object names, their spatial relationships, etc.) and prompts GPT-4 (or a smaller variant dubbed “GPT-4o-mini”) to generate a helpful description. The resulting text is then converted to speech and delivered to the user. This design is powerful because it leverages the **knowledge and linguistic ability of an LLM** to provide context (“You are in a kitchen. I see a person standing near the refrigerator and a dog lying on the floor.”) rather than just raw labels. It effectively brings in commonsense reasoning and can even answer follow-up questions. The downside, of course, is dependence on an Internet connection and the cost of API calls. But as LLMs get optimized or fine-tuned for on-device use, we may soon see local models providing similar functionality.

Another aspect of contextual understanding is **personalization**. A generic object detector might say “unknown person” for a face, but if the system can recognize *who* that person is (from a learned face database) or *what* a particular object means to the user, the feedback becomes far more meaningful. AIris has a face recognition module that can learn new faces (e.g. family members) and then announce “John is here” instead of just “a man is here”. Baig’s system includes a *Personalized Recognition Database* where users can easily add new objects or people by a one-click operation, after which the system will identify those specific items by name. This continual learning capability improves accuracy over time and adapts to the user’s life (for example, recognizing the user’s guide dog or frequently used objects at home). It also addresses one limitation of deep models – they tend to detect only the classes they were trained on. By allowing user-defined classes, the assistive device becomes more **customized and extensible**.

Finally, beyond vision, incorporating **additional context sensors** enhances understanding. Depth or distance sensors (ultrasonic, LiDAR) can tell how far an object is, allowing the system to warn about imminent obstacles (e.g. Baig’s hat triggers a buzzer if an object is <20 cm away). GPS provides outdoor location context, enabling the system to perhaps fetch location-specific information or assist in wayfinding (as MagicEye does). In sum, real-time object detection provides the *building blocks of scene understanding* by naming and locating entities, and this is enriched by higher-level vision-language models that explain the scene, plus personalization and sensor data to ground the information in the user’s context. The convergence of these allows systems like AIris to move from simply saying “car, tree, person” to conveying messages like “**There is a car parked on your left and a person approaching you on the sidewalk**” – a level of helpful detail that can truly transform a visually impaired user’s experience of the world.

## Text-to-Speech and Audio Output for Accessibility

The final step in any assistive vision system is delivering information to the user in an accessible format – usually, **speech audio**. Thus, **text-to-speech (TTS)** technology is an integral component. Early devices often relied on robotic but offline TTS engines such as *eSpeak* or *Festival*, which can run on lightweight hardware. For instance, Baig’s prototype uses the Festival TTS engine on the Raspberry Pi to convert the generated descriptions into speech locally. Festival is free and lightweight, though the voice is somewhat mechanical. The advantage of a local TTS is that it works without internet and with minimal latency. AIris does not explicitly state which TTS is used for output, but given that it aimed to be low-cost and offline, it likely used an open-source engine or the OS’s built-in speech synthesizer. In scenarios where the device is already connecting to a server, **cloud-based TTS services** (like Google Cloud Text-to-Speech or Amazon Polly) become viable – these provide very natural-sounding voices but require connectivity. For example, if AIris is sending images to a cloud server, that server could also generate the speech audio using a high-quality model and stream it back. However, doing TTS on the server might add to latency; a hybrid approach might cache common phrases or use an onboard TTS for faster response.

Two important considerations for audio output in wearables are **latency** and **audibility**. Latency is critical: the user might be interacting with a dynamic environment (crossing a street or someone approaching) and needs information promptly. Any lag between camera capture and spoken feedback can reduce usefulness or even safety. AIris testers indeed pointed out that reducing response latency is crucial for real-world use. Ensuring the TTS conversion is fast (and that audio starts playing as soon as possible, perhaps even while still streaming in) is part of that optimization. The other factor, audibility, concerns how the user hears the information. Many devices use **bone-conduction headphones or earpieces**, which transmit sound through the skull bones rather than the outer ear. This has the huge benefit of leaving the ears open to ambient sounds – visually impaired individuals rely heavily on hearing for awareness (traffic noise, people’s voices, etc.), so it’s important not to block their ears. Baig’s system uses bone-conduction earphones so that the user can hear the device’s voice clearly while still being alert to surrounding sounds. Similarly, OrCam’s product and others use small speakers positioned near the ear. Volume and voice clarity must be adjustable; some users may prefer a certain voice gender or style, or need louder output in noisy environments. User feedback from AIris indicated a desire for more *personalized settings* in audio output, such as speech rate and verbosity.

It’s also worth noting that **speech-to-text (STT)** plays a role when voice commands are used. AIris, for example, supports voice queries – presumably using Google’s Speech Recognition API for parsing user speech (they report \~95% accuracy in English STT). A reliable STT interface allows the user to ask questions or give commands (“describe the scene”, “read this document”) without needing to press buttons. Combined with TTS feedback, this creates a **closed-loop voice interaction** that is natural for users. We see this in NewVision’s concept where a voice assistant handles queries. Ensuring that the system can discern the user’s voice amid background noise (perhaps using a lapel microphone or filter) is a practical concern for deployment.

In summary, robust TTS is what ultimately communicates the AI’s understanding back to the user, effectively becoming the “voice” of the AI assistant. Advances in TTS (like neural transformers that produce very humanlike speech) are making their way into embedded devices, though often requiring some acceleration. For now, many projects rely on tried-and-true engines like Festival for offline use or leverage cloud TTS when connectivity allows. The **goal is to provide clear, timely, and non-intrusive audio** that the visually impaired user can comfortably rely on in all environments.

## Conclusion and Outlook

The convergence of wearable hardware, computer vision, and language AI is enabling a new generation of assistive devices like **AIris** that can dramatically improve day-to-day life for visually impaired individuals. In the past five years, research has progressed from single-function tools (e.g. obstacle detectors or text readers) to comprehensive assistants that attempt to **understand and verbalize the user’s entire scene**. Key developments underpinning this progress include: powerful yet efficient vision models for real-time object detection (e.g. YOLO, MobileNet-SSD), the emergence of large vision-language models capable of rich image descriptions (BLIP-2, MiniGPT-4, LLaVA), and practical system integrations on embedded platforms (Raspberry Pi 4, Jetson) with creative partitioning between edge and cloud processing. Early prototypes have shown success in real-world trials – users report greater environmental awareness, confidence in navigation, and improved ability to perform daily tasks with these aids.

That said, there remain challenges and avenues for further research. **Latency and real-time performance** are critical – future work will likely explore model compression, hardware acceleration, and 5G/edge computing to minimize the delay from perception to feedback. **User experience and form factor** are equally important: devices must be comfortable, unobtrusive, and easy to operate for extended periods. This may drive innovations in battery technology, heat management, and ergonomic industrial design (lighter glasses, fashionable designs, etc.). **Personalization and adaptability** will also be a focus – systems might learn a user’s routines, familiar places, and preferences over time to tailor the information (for example, warning not just of “a car” but that “the blue car of your neighbor is parked in your driveway”). On the algorithmic front, integrating **multi-modal inputs** (vision, sound, GPS, inertial sensors) can yield a more robust understanding of context; for instance, hearing a honk could cue the system to warn of traffic even before the camera sees it.

Importantly, the literature indicates a trend towards **using large pre-trained models in assistive tech**, which raises considerations of privacy (streaming camera data to cloud) and cost. Researchers are investigating on-device large-model inference and distilled models so that eventually a pair of smart glasses could run a smaller version of something like GPT-4 vision internally. The AIris team’s plan to incorporate LLMs and generative AI into the next version of their device highlights this trajectory. The international research community – as seen in journals like *IJCAET* and *JECE* – is actively sharing findings from user studies, technical benchmarks, and novel assistive algorithms, ensuring that progress is disseminated across disciplines.

In conclusion, the **state of the art** in AI for visually impaired assistance is advancing rapidly on multiple fronts. Vision-language models provide unprecedented descriptive power; wearable computing brings those models into users’ daily lives; and edge AI techniques make it feasible to deliver results in real time. The AIris project sits at this intersection, and the literature reviewed here affirms its approach while suggesting pathways for enhancement. With continued research and cross-pollination between fields (computer vision, HCI, embedded systems, accessibility studies), AI-powered wearables are on the path to becoming an invaluable companion for the blind – effectively a *pair of eyes* that see and speak about the world around them. **Such systems are not only technologically exciting but have profound social impact**, promising greater independence and inclusion for millions of visually impaired people worldwide.

**Sources:**

* Brilli, D. D. *et al.* (2024). *AIris: An AI-powered Wearable Assistive Device for the Visually Impaired*.&#x20;
* Baig, M. S. A. *et al.* (2024). *AI-based Wearable Vision Assistance System... Using Large Vision-Language Models*.&#x20;
* Sethuraman, S. C. *et al.* (2023). *MagicEye: An Intelligent Wearable Towards Independent Living of Visually Impaired*.&#x20;
* Bobba, K. S. *et al.* (2023). *NewVision: Helping Blind People Using Deep Learning*.&#x20;
* Li, J. *et al.* (2023). *BLIP-2: Bootstrapping Language-Image Pre-training...*.&#x20;
* Zhu, D. *et al.* (2023). *MiniGPT-4: Enhancing Vision-Language Understanding...*.&#x20;
* Liu, H. *et al.* (2023). *Visual Instruction Tuning (LLaVA)*.&#x20;
* Additional references on assistive tech surveys, smart canes, etc.: , and others as cited above.
