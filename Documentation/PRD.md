# AIris Product Requirements Document (PRD)

## Document Information
- **Product Name**: AIris â€” AI-Powered Vision Assistant
- **Version**: 2.0
- **Date**: December 2025
- **Project Phase**: CSE 499A/B Academic Project

---

## Product Overview

### Vision Statement
AIris is a wearable AI-powered vision assistant that helps visually impaired users navigate their environment and locate objects through real-time audio feedback.

### Problem Statement
Current visual assistance solutions suffer from:
- High latency (>5 seconds response time)
- Cloud dependency and privacy concerns
- Smartphone-dependent interfaces not accessible to blind users
- Lack of active guidance for object localization
- Limited real-time capabilities

### Solution
A purpose-built wearable device providing:
- **Active Guidance** â€” Audio instructions to find and reach specific objects
- **Scene Description** â€” Continuous environment awareness with safety alerts
- **Wireless Design** â€” ESP32 camera (WiFi) + Arduino audio (Bluetooth)
- **Privacy-First** â€” All AI processing on user's local server
- **Hands-Free** â€” Physical buttons, no screen interaction required

---

## System Architecture

### Hardware Components

| Component | Specification | Purpose |
|:----------|:--------------|:--------|
| **Camera** | ESP32-CAM | Video capture, WiFi streaming to server |
| **Audio Input** | Microphone via Arduino | Voice commands from user |
| **Audio Output** | Speaker via Arduino | Audio feedback delivery |
| **Wireless** | WiFi (camera), Bluetooth (audio) | Cable-free operation |
| **Processing** | Server/Computer | AI inference, backend services |
| **Controls** | Physical buttons | Mode selection, activation |

### Software Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIris Software Stack                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Backend Services (FastAPI)                            â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ Camera Service      â€” Video feed handling         â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ Model Service       â€” YOLO, MediaPipe, BLIP       â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ Activity Guide      â€” Object localization logic   â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ Scene Description   â€” Environment analysis        â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ STT Service         â€” Whisper speech recognition  â”‚ â”‚
â”‚  â”‚  â””â”€â”€ TTS Service         â€” Audio response generation   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  AI Models                                              â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ YOLOv8              â€” Real-time object detection  â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ MediaPipe           â€” Hand tracking               â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ BLIP                â€” Image captioning            â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ Groq API            â€” LLM reasoning (Llama 3)     â”‚ â”‚
â”‚  â”‚  â””â”€â”€ Whisper             â€” Speech-to-text              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Frontend (React) â€” Development GUI                    â”‚ â”‚
â”‚  â”‚  Note: Proof of concept only. Final device uses        â”‚ â”‚
â”‚  â”‚  physical buttons + audio, no screen required.         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Functional Requirements

### FR-1: Active Guidance Mode

**Description**: Guide user to locate and reach a specified object.

**Requirements**:
- User speaks object name (e.g., "find my water bottle")
- System detects object using YOLO
- System tracks user's hand using MediaPipe
- LLM generates directional instructions ("move left", "reach forward")
- Audio feedback continues until hand reaches object

**Acceptance Criteria**:
- âœ… Object detection accuracy >85%
- âœ… Hand tracking works reliably
- âœ… Audio instructions are clear and actionable
- âœ… System confirms when object is reached

### FR-2: Scene Description Mode

**Description**: Provide continuous environment awareness.

**Requirements**:
- Analyze video feed using BLIP vision model
- Generate contextual descriptions via LLM
- Prioritize safety-relevant information
- Support guardian alert notifications

**Acceptance Criteria**:
- âœ… Descriptions are contextually relevant
- âœ… Safety hazards are identified and prioritized
- ğŸ”„ Guardian alerts functional (in testing)

### FR-3: Voice Interaction

**Description**: Hands-free voice command and response.

**Requirements**:
- Speech-to-text using Whisper
- Text-to-speech for audio responses
- Support via Arduino Bluetooth audio

**Acceptance Criteria**:
- âœ… Voice commands recognized accurately
- âœ… Audio responses are clear
- ğŸ”„ Bluetooth audio integration (in progress)

### FR-4: Wireless Operation

**Description**: Cable-free wearable design.

**Requirements**:
- ESP32-CAM streams video over WiFi
- Arduino handles audio over Bluetooth
- Physical buttons for basic controls

**Acceptance Criteria**:
- ğŸ”„ WiFi streaming functional (in progress)
- ğŸ”„ Bluetooth audio functional (in progress)
- â³ Button controls (pending)

---

## Technical Requirements

### TR-1: Software Dependencies

```yaml
Backend:
  - Python 3.10+
  - FastAPI
  - PyTorch
  - Ultralytics (YOLOv8)
  - MediaPipe
  - Transformers (BLIP)
  - OpenAI Whisper
  - pyttsx3
  - Groq SDK

Frontend:
  - Node.js 18+
  - React
  - TypeScript
  - Vite
  - Tailwind CSS

Hardware Firmware:
  - Arduino IDE
  - ESP32 libraries
  - Bluetooth libraries
```

### TR-2: Performance Targets

| Metric | Target | Current Status |
|:-------|:-------|:---------------|
| **Guidance Response** | < 2s | âœ… Achieved |
| **Object Detection** | > 85% accuracy | âœ… Achieved |
| **Scene Description** | < 5s | ğŸ”„ Testing |
| **Voice Recognition** | > 90% accuracy | âœ… Achieved |

---

## User Experience Requirements

### UX-1: Accessibility First
- No screen interaction required for core functions
- Physical buttons for mode selection
- Clear, concise audio feedback
- Consistent audio cues for system states

### UX-2: Hands-Free Design
- Wearable camera (spectacle-mounted or clip-on)
- Wireless audio (earpiece or speaker)
- No cables during operation

### UX-3: Safety Prioritization
- Hazard detection in Scene Description mode
- Guardian notification system for emergencies
- Clear "obstacle ahead" type warnings

---

## Development Status

### Completed âœ…
- Core software architecture
- Active Guidance mode implementation
- Backend API and services
- Frontend development interface
- YOLO object detection integration
- MediaPipe hand tracking
- Groq LLM integration
- Whisper speech-to-text
- pyttsx3 text-to-speech

### In Progress ğŸ”„
- Scene Description mode refinement
- ESP32-CAM WiFi integration
- Arduino Bluetooth audio
- Guardian alert system

### Pending â³
- Physical button controls
- Wearable enclosure design
- User field testing
- Final documentation

---

## Success Criteria

### MVP Requirements
1. **Active Guidance**: User can find objects using voice commands
2. **Scene Description**: Continuous environment awareness
3. **Wireless Operation**: ESP32 camera + Arduino audio working
4. **Standalone Use**: No screen required for blind users

### Quality Metrics
- Guidance accuracy: >85% success rate
- Response latency: <2 seconds
- User satisfaction: Positive feedback from testing

---

## Future Considerations

### Potential Enhancements
- Facial recognition for people identification
- OCR for text reading
- Multi-language support
- Mobile companion app

### Scalability
- Cloud backup for complex scenes
- Remote guardian dashboard
- Community-shared location descriptions

---

*This PRD serves as the guide for AIris development. Implementation details are in the codebase at `/AIris-System/`.*
