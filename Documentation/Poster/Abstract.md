# Abstract

Visually impaired individuals face significant challenges in independently navigating their environment and locating objects. Existing assistive technologies often suffer from high latency, reliance on unstable cloud connections, and a passive approach that describes scenes without offering actionable guidance. **AIris** addresses these limitations by introducing a real-time, computer-based AI assistant designed for active interaction. By integrating state-of-the-art computer vision models (**YOLO26s** for object detection, **MediaPipe** for hand tracking, and **BLIP** for scene analysis) with a robust LLM reasoning engine (**GPT OSS120B via Groq API** for object extraction and summarization), AIris provides two core modes: **Active Guidance**, which directs the user's hand to specific objects with precise audio cues using a novel rule-based geometric algorithm (no LLM required for guidance generation), and **Scene Description**, which offers continuous environmental awareness with advanced fall detection and automated guardian email alerts. The system features complete handsfree voice control via **Web Speech API** (native browser STT/TTS), prioritizes privacy through local-first processing, and achieves sub-2-second response times via low-latency cloud inference, making real-time assistance a practical reality.
