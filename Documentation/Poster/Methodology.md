# Methodology

The AIris system operates on a sophisticated multi-stage pipeline designed to mimic human visual processing and reasoning.

### 1. Dual-Mode Operation
*   **Active Guidance Mode (Targeted):** When a user asks to find an object (e.g., "Find my water bottle"), the system uses **GPT OSS120B** (Groq API) to extract the target object from the natural language goal. The system then activates **YOLO26s** (Ultralytics) to locate the target in real-time with high precision. Simultaneously, **MediaPipe** tracks the user's hand coordinates using 21 landmark points. A proprietary geometric algorithm calculates the 3D spatial vector between the hand centroid and object bounding box center, and estimates relative depth using bounding box size ratios. A novel rule-based algorithm (no LLM) converts this vector and depth information into natural language directional commands, generating precise audio instructions ("Move slightly right and forward") in real-time until hand-object contact is confirmed through distance and depth threshold analysis.
*   **Scene Description Mode (Continuous):** For general awareness, the system employs **BLIP** (Bootstrapping Language-Image Pre-training) to capture dense semantic captions of the video feed at 2 FPS. These captions undergo quick keyword-based risk assessment, static frame detection (for fall indicators), and transition pattern analysis. Frame descriptions are accumulated in a buffer (5-10 frames) and processed by **GPT OSS120B** on the Groq fast inference engine to generate contextual summaries, calculate multi-factor risk scores, and detect potential falls through multi-method analysis. The system includes automated guardian email alerts with configurable risk thresholds (0.1-0.5) and provides daily/weekly activity summaries.

### 2. Privacy-First Architecture
Unlike cloud-heavy alternatives, AIris is engineered for local-first execution. Deep learning inference (Objects, Hands, Speech) runs on the local server. Only complex semantic reasoning is offloaded to the high-speed Groq API, ensuring that video feeds are processed fleetingly without permanent storage, protecting user privacy.

### 3. Human-Computer Interaction (HCI)
The interface is entirely screen-free through **Handsfree Mode**. Users interact via **natural voice commands** processed by **Web Speech API** (native browser speech recognition). The system supports voice commands for mode switching, camera control, task input, and feedback. All instructions, descriptions, and confirmations are automatically delivered through **Web Speech API** (native browser speech synthesis), creating a seamless conversational loop that requires no tactile interaction. The system works with any device (computer, phone, tablet) and optional hardware accessories (custom ESP32-CAM with casing, Bluetooth headset) for enhanced handsfree operation.
